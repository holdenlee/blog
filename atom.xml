<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Mental Wilderness</title>
    <link href="http://holdenlee.github.io/blog/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/blog" />
    <id>http://holdenlee.github.io/blog/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2019-06-11T00:00:00Z</updated>
    <entry>
    <title>Month of Writing, Part 0</title>
    <link href="http://holdenlee.github.io/blog/posts/writing/month_of_writing_part_0.html" />
    <id>http://holdenlee.github.io/blog/posts/writing/month_of_writing_part_0.html</id>
    <published>2019-06-11T00:00:00Z</published>
    <updated>2019-06-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Month of Writing, Part 0</h1>
    </div>
    
    <div class="info">
       <div class="subtitle"><p>Starting out</p></div> 
       
        <p>Posted: 2019-06-11 
          , Modified: 2019-06-11 
	</p>
      
       <p>Tags: <a href="/tags/fiction.html">fiction</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>I’m spending one month focusing on writing! If you want to stay updated on my work, you can request to join the <a href="https://groups.google.com/forum/#!forum/holden-readers">Holden’s Readers googlegroup</a>.</p>
<h2 id="where-i-am-now">Where I am now</h2>
<p>After five years of work, I finally received my Ph.D. in mathematics! During grad school, I’ve put aside my writing for extended periods of time. But no more; I’ve come to realize over the past few years how essential writing is for me. Some of the most blissful times I’ve had are when I’m writing; moreover, my writing ambitions are no less important to me than my academic ones.</p>
<p>I’m thankful to the communities of writers I have been part of over the past two years, who have helped me to realize this. I’m indebted to my friend who started <a href="https://archandarrow.princeton.edu/">Arch and Arrow</a>, a writing group at Princeton because she was “tired of writing alone in her room.” I’ve been inspired by the writing meetups I participated in during my summer in New York, which I wrote about <a href="https://docs.google.com/document/d/1gBfuIygKNVX0Sx3o57cnzgejif76LpnxWjSkA70G3YM/edit">here</a>. It’s really made me see the value of having a community of like-minded people. If you are an aspiring writer I encourage you to check out a meetup in your area, or otherwise find some friends you can share writing with.</p>
<p>This spring I applied to three fiction workshops (Clarion, Clarion West, Odyssey). I was disappointed that I didn’t get in any of them, but the process of applying forced me to revise some of my stories, to reflect on my progress as a writer, and think about what my next steps are. So I’ve decided to take a month off from work to focus on my writing. I’ll mainly be working on stories, and also a few blog posts.</p>
<h2 id="goals">Goals</h2>
<p>My concrete goal is to write three short stories, and get them to a point where I feel good about sharing them with other people. I hope to do some revision, but it’s also OK to revise in the following weeks. I’ll focus on short stories because I can iterate much faster - I can learn from the previous one as I start the next one, and I can write stories that are different from each other to stretch my abilities. While I enjoy working on a novel more than a short story, I think the practice from writing more short stories will be more helpful at this time.</p>
<p>I want this month to not just be focused of finishing stories, but instead on being a writer more broadly:</p>
<ul>
<li>reading a lot of material</li>
<li>recording what I learn from reading</li>
<li>improve at particular aspects of the craft (through specific writing exercises and lessons)</li>
<li>generate and execute on a lot of ideas (I want to brainstorm lots of ideas and test-write some of them; this way I will have a lot of threads I could draw on later)</li>
<li>focus on making progress on a specific short stories/novella/novel</li>
<li>do research as necessary</li>
<li>becoming confident about being a writer (not feeling like an impostor)</li>
<li>becoming articulate about writing</li>
<li>engage with other writers (ex. through exchanging and editing each other’s work)</li>
<li>sharing through email and blog</li>
<li>submitting stories for publication</li>
</ul>
<p>I want to expand the things I write about, and write what I <em>actually</em> want to write. There are some persistent themes in my writing (rational/optimization-minded people becoming disillusioned, or having the tables turned on them in terms of what’s important or valuable in life) that I would rather escape from. I was drawn to those themes as a response to the pressure from work, and they are not the bigger, more beautiful things I would write about if I were in a better state of mind.</p>
<h2 id="the-bigger-picture">The bigger picture</h2>
<p>More than just putting in a lot of hours, I want to develop good reading and writing habits that will persist afterwards. For example, I want to “garden” my thoughts, develop thoughtful reactions to things I read, and be able to talk about them.</p>
<p>Even more broadly, I look forward to a month of self-reflection, discovery, and improvement. The practice of writing for me is more entangled with personal growth than math or computer science research is. My hope is that in freeing up the large amount of time that I would normally be working, that I can devote my full intellectual energy to things that have fallen by the wayside, instead of just the dregs of the energy at the end of the day.</p>
<p>Stay tuned for updates!</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>The Bookmaking Species, Ken Liu</title>
    <link href="http://holdenlee.github.io/blog/posts/talks/the_bookmaking_species.html" />
    <id>http://holdenlee.github.io/blog/posts/talks/the_bookmaking_species.html</id>
    <published>2018-08-15T00:00:00Z</published>
    <updated>2018-08-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>The Bookmaking Species, Ken Liu</h1>
    </div>
    
    <div class="info">
       <div class="subtitle"><p>The evolution of books; how form shapes narrative</p></div> 
       
        <p>Posted: 2018-08-15 
          , Modified: 2016-08-15 
	</p>
      
       <p>Tags: <a href="/tags/books.html">books</a>, <a href="/tags/writing.html">writing</a>, <a href="/tags/story.html">story</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#summary-and-takeaways">Summary and takeaways</a></li>
 <li><a href="#history-of-books">History of books</a><ul>
 <li><a href="#from-scroll-to-codex">From scroll to codex</a><ul>
 <li><a href="#the-west">The West</a></li>
 <li><a href="#east-asia">East Asia</a></li>
 </ul></li>
 </ul></li>
 <li><a href="#how-structure-influences-narrative">How structure influences narrative</a><ul>
 <li><a href="#webpages">Webpages</a></li>
 <li><a href="#aside---books-in-the-legal-profession">Aside - books in the legal profession</a></li>
 <li><a href="#e-books">E-books</a></li>
 </ul></li>
 <li><a href="#the-untapped-potential-of-electronic-reading-and-the-memex">The untapped potential of electronic reading, and the memex</a></li>
 <li><a href="#questions-and-answers">Questions and answers</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>These notes are based on <a href="https://kenliu.name/">Ken Liu</a>’s lecture “The bookmaking species” at the Swiss Institute. I’ve loosely paraphrased and condensed his talks; any errors or misrepresentations are due to me.</p>
<h2 id="summary-and-takeaways">Summary and takeaways</h2>
<ul>
<li>There’s a fascinating history behind the shift from scrolls to codices (the bound book format we are familiar with) that happened in both the West and East; it’s influenced by both technological and cultural/religious factors.</li>
<li>The structure of a book, as well as economic, historical, and cultural factors of the times, shapes the stories that gets told. The types of stories evolve - for example, TV shows nowadays have more complex plots than ever before.</li>
<li>The potential of electronic books for new types of storytelling, and better ways of recording and interacting with knowledge, has been largely untapped. Instead, e-books have mostly been formatted to imitate traditional books. As it is now, the Internet is a pale imitation of Vannevar Bush’s vision of the memex augmenting human intelligence.</li>
<li>Storytellers (writers, film producers, etc.) like to think that their profession is one of the most immune to the machine learning/AI revolution. However, machine learning is already playing a part in helping artists cater to what the public desires, and its role will only increase in the future.</li>
</ul>
<h2 id="history-of-books">History of books</h2>
<p>I’m interested in books as artifacts, the history of books, and how the type of book changes the story that is told.</p>
<p>(One cool book is <a href="http://visual-editions.com/tree-of-codes">Tree of Codes</a> by Jonathan Foer, which creates a new book by cutting out pieces from an old book.)</p>
<p>What is a book? The familiar bound and paginated book is only one type of book, known as the <strong>codex</strong>. Before that was the <strong>scroll</strong>. Books can come in many forms: the earliest were oracle bones, with characters written for divination; other forms are cuneiform, papyrus, bamboo strips, and Incan <a href="https://en.wikipedia.org/wiki/Quipu">quipus</a>. Now we have e-books. (I used them long before they were popular. The <a href="https://wiki.mobileread.com/wiki/Rocket_eBook">rocket e-book</a> was the earliest model. To download a good-sized book you had to plug in a phone cable and wait ten minutes).</p>
<p>The scroll came before the codex, and for a long time was the only format, or the more prestigious format. Scrolls were considered more intellectual - for instance, diplomas were depicted as scrolls.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Dead_Sea_Scrolls">Dead Sea Scrolls</a> had aspects of the codex: the text was divided into pagelike columns, and so it could be folded concertina-style.</p>
<p>Legend has it (probably apocryphal) that Julius Caesar was the first to fold scrolls because he wanted to take books with him during the first Gaelic War.</p>
<h3 id="from-scroll-to-codex">From scroll to codex</h3>
<p>How did the codex overtake the scroll? There are two main reasons, technological and cultural/religious, and a similar story played out in both the western and eastern world.</p>
<h4 id="the-west">The West</h4>
<p><strong>Technological:</strong> It wasn’t until 200-300 AD that the codex became dominant. It’s no accident that this coincided with Christianity becoming the state religion, and the dominant religion, in the Mediterranean.</p>
<p>The codex had enormous advantage over the scroll as a medium for Christians to spread their teachings. A codex is a random access medium: you can instantly flip to any page. A codex allows innovations such as page numbers, indices, and tables of contents. Codices are easier to copy and produce. They are compact and portable, utilizing both sides of each page.</p>
<p><strong>Cultural:</strong> But this is not the whole story. Codices existed before they became popular; why didn’t they become the standard given their obvious benefits?</p>
<p>The earliest codices were formed from bundles of letters - people saved letters from loved ones who lived far away, and bound them together. Thus codices had a mundane association to them: they were not prestigious, nor philosophical, like scrolls; they were like paperback is to hardcover today.</p>
<p>The Christian scriptures, however, were in the form of letters, so it was natural to follow the tradition of binding them into a codex. In addition, Christians wanted to distinguish themselves from Jews, whose holy book, the Torah, is the quintessential scroll.</p>
<h4 id="east-asia">East Asia</h4>
<p>The word for volume, 冊 is in the shape of two strips of bamboo bound together.</p>
<p>As in the West, scrolls were the prestige format, seen as ancient and powerful (and still are - people do calligraphy on scrolls, not in codices). Paintings were done on scrolls. (In museums they are laid completely flat, to be viewed as static, unscrolled objects, but this is not how they were originally seen. They were unrolled so that viewers saw a window at a time.)</p>
<p>The codex was invented in China in the 7th century AD.</p>
<p><strong>Technological:</strong> The codex rose to prominence in the Tang dynasty, the era of classical poetry. Back then, composing poems was the way to show you were smart (like op-ed medium think pieces nowadays). You hoped that your poem would go viral, and be heard by government officials.</p>
<p>To compose poems, people needed rhyming dictionaries, and codices are undoubtedly the right form for dictionaries. (Imagine trying to use a rhyming dictionary in a scroll format!)</p>
<p><strong>Cultural/religious:</strong> One of the largest scale mass translations in ancient times was that of Buddhist texts from India into Chinese. The texts were written on palm leaves and bound using strings. As they were holy books, people wanted to preserve the format as closely as possible. China didn’t have palm leaves, but had plenty of paper - the Chinese analogue of palm leaves - so they made the texts into paper connected via string. The Chinese word for page was the same as the Chinese word for leaf (葉) until it got changed to 頁 in Republican times.</p>
<h2 id="how-structure-influences-narrative">How structure influences narrative</h2>
<p>A book is not merely the abstract set of words that is contains. The book as physical artifact influences the story that it tells. The modern novel is 350 pages long because 350 pages is a standard book size. Epic fantasies come in trilogies not for the reasons that authors come up with afterwards (the first book is the thesis, the second the antithesis, the third the synthesis) but because this is what publishers think what readers that. This trend originated with Lord of the Rings, which ironically was written as one long book and only split into three books because it was too large.</p>
<p>E-books have given rise to lengths that were not popular before: both shorter novels (~40,000 words) which were not viable in the familiar format, and larger novels (600,000 to 1 million words) which can’t be bound into traditional books. There is no narratological reason that books have to be a certain length.</p>
<p>The Chinese e-book market is dominated by very long serial novels. There is intense competitions between authors. They gain readers by posting serial installments for free, and have to write 2000 characters every day - often including holidays - to be competitive. (Dickens also wrote in serial installments; when he took a week off he got death threats.) The most successful stories become TV dramas and are turned into print editions (though they are so long I don’t know who would buy them).</p>
<p>Modern storytelling has changed because of Game of Thrones. TV shows (e.g. Friends) used to be made up of episodes which could stand on their own, so one could jump in anytime. Ever since Game of Thrones, however, we have wanted long, complex stories that last the whole season and that we can watch in binges. <!-- soap opera--> We are in a unique age which has the most complicated narrative stories that humankind has ever written. There is no way to follow Lost without wikis, and even George R.R. Martin says that he relies on fan wikis.</p>
<h3 id="webpages">Webpages</h3>
<p>The scroll has returned in the digital age, as the format for websites. People get mad if a website is broken up into pages - is the website just trying to increase ad views?</p>
<p>Early on, people figured out how to modify the CSS to get infinite scroll; it was the cool thing to do on tumblr.</p>
<p>However, history was not so certain. There are two models for hypertext, the scroll model and the card model. The card model has breaks the text into smaller chunks with links between them. One reason the scroll model won is that it has all the benefits of the scroll and the codex: with anchors and links, clickable table of contents, and indices produced by search.</p>
<p>Ironically, however, although the scroll has cultural significance as an authoritative relic, it is the written page that has hold on us now - as something more romantic than electronic text.</p>
<h3 id="aside---books-in-the-legal-profession">Aside - books in the legal profession</h3>
<p>As a lawyer, one of my angsts against the legal profession is its formal (“blue book”) citation method. It’s a system tied to hardcover books. Walk into a lawyer’s office and you’ll see bookshelves, one on either side, like a repository of arcane wizarding knowledge.</p>
<p>It’s a stage prop. I’ve consulted the books about twice in my career.</p>
<p>Instead, lawyers use legal databases to find information. In the databases, the text is in the form of an infinite scroll, but with ugly symbols denoting the page numbers, etc. It persists as a fiction that lawyers are consulting paper; the physical books are authoritative even though nobody has been using them for decades.</p>
<h3 id="e-books">E-books</h3>
<p>Kindle books are imitating the codex - for example in the ability to have page numbers (useful for citing). Apple is the worst offender, it has a “bookshelf” and when you flip the page there is an animation like an actual page flipping.</p>
<p>Codices aren’t any good when they only imitate scrolls. Likewise e-books aren’t any good if they just imitate paper books.</p>
<p>The New Yorker app reimagines what a electronic magazine can look like. It has articles side by side, and each article is an infinite scroll; it effectively utilizes a 2-D grid.</p>
<p>There’s great potential for hypertext fiction. We haven’t reimagined navigation yet.</p>
<h2 id="the-untapped-potential-of-electronic-reading-and-the-memex">The untapped potential of electronic reading, and the memex</h2>
<p>Vannevar Bush, in his essay <a href="https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/">As we may think</a>, envisioned the <em>memex</em>, a most wondrous device for electronic reading. It’s commonly described as the predecessor of the Internet, but I disagree: it’s far more impressive.</p>
<p>Machines have helped us amplify our physical capabilities, but they have been much more limited in amplifying our mental capabilities. One difficulty is that how we think is different from how computers think: humans are association machines, thinking in metaphors and accessing thoughts in a random fashion. Why not use computers to enhance this style of thinking? In Vannevar Bush’s vision, a memex features two screens to display text, and a user can tap to make a connection between them. He imagines that the primary work of the reader is to make these kinds of rich semantic links. The resulting database of links becomes a external manifestation of the reader’s mind.</p>
<p>It is much easier for us to recall something by following associations than by looking up a random word. For example, how many times have you thought: I wish I could recall that website - I saw it after this and before that, and it reminds me of this, but I can’t remember the words I used to find it…</p>
<p>Interested (and interesting) explorers of the universe and of human learning will forge trails of associations through all knowledge. A person could jump from the Origin of Species to Darwin’s original papers, to finches, to customs of the natives, to British politics, and so forth, to construct a natural history of a subject within the archive of human learning. The resulting rich web of links can be published as a book of its own, so that readers can follow it and build their own web on top of it, to create meta-associations. This will make learning much more personal and guided in a way we don’t really comprehend. The closest thing that we have now is browsing Wikipedia. Imagine if all learning were a richer form of Wikipedia where you could lose yourself in association. Authors could become known for the originality and beauty of their association webs. <!--(You would build on top of that.)--> <!--That's the true vision of the memex, and --> You would build on top of them like a <a href="https://en.wikipedia.org/wiki/Commonplace_book">commonplace book</a>, except that it’s a commonplace web. The Web is a poor and pale imitation of this future!</p>
<p>As technology changes the kinds of books that are possible, the stories we can tell also change, and that’s a wonderful thing. I don’t believe we should be stuck with the scroll or codex. I can’t wait to see how electronic reading will develop as we push forward. I don’t lament the death of the paper book; what will come to replace it will be far more interesting.</p>
<h2 id="questions-and-answers">Questions and answers</h2>
<p><strong>Q:</strong> A lot of bookstores are having trouble. What’s the future for bookstores?</p>
<p><strong>A:</strong> Bookstores aren’t going away, though chains have been hit hard. But they cannot compete with indie bookstores, which are community centers with author events; their curatorial role will not go away. Individual readers do wish to participate in the events and support the bookstore. The general trend is that content is digitized and turned into advertising for things can’t be easily copied.</p>
<p>A similar thing has happened in the music industry. Here, the thing that can’t be easily copied is performance. Authors have not quite figured out what “performance” is for them. (It’s incredibly boring to watch an author write a book.) Musicians thought digitization was the end of the world, but now there is now more music than ever before, and many musicians are living better than ever, though music is being sold for cheaper. It’s not true for everyone though - musicians who don’t rely on live performances don’t do as well - it does threaten certain business models.</p>
<p><strong>Q:</strong> What are some books that push these new frontiers?</p>
<p><strong>A:</strong> I haven’t seen them so much in books as in games: visual novels (essentially books with programming around them) are pushing narrative in new interesting directions.</p>
<p>I’d like to see more experimentation. Some publishers are doing interesting things, like <a href="https://www.serialbox.com/">serialbox</a>, who treats creating a serial story like creating a TV show, with writer’s rooms and seasons of stories. It’s an interesting model for book-writing.</p>
<p><strong>Q:</strong> Have you taken this into account in the format of your own books?</p>
<p><strong>A:</strong> I always envision them as physical books, with a map at the front, so that the reader can flip to it as they are reading. However, in the e-book it’s hard to refer to the map. Ironically, I had been blind to this. I’ve made the map available for download, and encouraged readers to print it out.</p>
<p><strong>Q:</strong> How is writing versus translating?</p>
<p><strong>A:</strong> It’s like the difference between composing and performing music, writing a play and acting. You use similar knowledge to do different things.</p>
<p><strong>Q:</strong> What is your approach to writing?</p>
<p><strong>A:</strong> I do unusual things, collaborating with ballet troupes, with the Guggenheim, doing art based on alternate reality. I like to experiment. I wrote a story that progressively loses words, so that you come to a different conclusion about what it’s about in the later versions.</p>
<p>For this exhibition, I was approached by someone who wanted to use the physical artifact of books to illustrate an abstract concept. I was hired at my attorney rate to curate a book collection on what comes to mind when I think about the law in China. There’s a temptation to be political, but I have an aversion to that. There is nothing wrong with art as resistance, but it has to be open to interpretation.</p>
<!--10141-->
<p>Making art is like building a house. Certain houses are narrow and confining; unless the reader contorts themselves they don’t feel comfortable… But if the house has open spaces, allows you to walk around, then the reader is comfortable moving in it, exploring and discovering and making the house their own.</p>
<p>I like the second approach better, because I’m a deep believer in the reader response theory of literature. Art takes two people to create the work. The reader is never a passive participant, but an active meaning maker.</p>
<p>Before the text can be unpacked, it has to be packed with the reader’s assumptions and interpretive frameworks.</p>
<p><strong>Q:</strong> If the future is a manifestation of (seemingly) random associations, would that require an evolution of narrative itself?</p>
<p>Media theorists have studied and debated this forever. There’s always the argument that the long-form story has an important role in the way we think. This goes back to the days of Plato.</p>
<p>This argument over whether e-reading is rotting our brains is not a new argument. Plato railed against the techology of writing: it will rot our children’s minds, and won’t know how to conduct rhetoric, argue against the writers, tell truth from fiction.</p>
<p>As association-rich, interruption-rich reading becomes the norm, it will lead to a evolution in the way we judge and think about texts.</p>
<p>One of the most popular new forms of fiction right now is twitter/text fiction: stories told as a chain of fictional text messages. As an art form, this is very difficult to do, and I’ve been amazed at the results. We can have alternate reality games, challenging what narrative can do.</p>
<p>We’ll have to rethink narrative theories, but we’ve done this before. We thought <a href="https://en.wikipedia.org/wiki/Classical_unities">Aristotelian unities</a> were sacred, but movies, as a time random-access medium, have broken that apart.</p>
<!--so many different possib for narr 
will tech design drive narrative, -->
<p><strong>Q:</strong> How does technology influence narrative? Will the designers of technologies, creators of platforms determine what narratives are told? Or will the narratives people create push for technology to accommodate them?</p>
<p><strong>A:</strong> The future of art is going to be much more AI and audience-driven than we acknowledge. It’s mindblowing the way that AI has changed the way content is produced and consumed. For example, a lot of music in films is created by AI, but we don’t notice it. Even the way human creators create is shaped by AI: in a lot of visual art manipulation, the algorithms do a lot of the work and the human simply acts as a judge.</p>
<p>The story of Netflix shows that dramatic art is no exception. No one thought it would succeed as a studio because it was supposedly just a platform to distribute content. However, in seven years it has gone from nothing to a top studio, with some of the most popular shows on earth. It’s now considered great prestige if you as a showrunner can get a show with Netflix.</p>
<p>Their success has been possible because they are very data-driven. For example, their decision to pick up House of Cards was data-driven. They discovered that there would be a critical mass of people who are fans of Kevin Spacey, people who love David Fisher’s work, and who loved the original BBC series. They’ve learned that people don’t get hooked from the pilots, but only after 2-4 episodes. The platform is an unprecedented <a href="https://en.wikipedia.org/wiki/Panopticon">panopticon</a>: they track when you pause or skip a video, wait two days before watching the next one, can infer when you lose interest or find a series worth following. Never before has anyone been able to observe an audience at that level. They can use all this data to help produce popular shows.</p>
<p>An extreme example of data-driven content creation is pornography on demand. The industry keeps track of search queries and uses them to meet people’s demands. Pornography has embraced a data-driven approach because it is a limited format where the creative choices are small.</p>
<p>Data-driven authorship is a critical issue. A lot of artists have to be data-driven or be outcompeted by people who are.</p>
<p>Imagine a future where stories can adapt to individual readers based on what they want: if a reader wants a couple to be together, they will be; if they want a character to go away, then the character dies on the next page. Perhaps this is how people prefer their stories.</p>
<p>We treasure a Promethean vision of an artist, creating a masterpiece unsullied by what the public wants. However, there is a competing, utilitarian vision of the artist, enshrined in the constitution, as someone who fulfills a public need. Indeed, mass film and TV have been largely driven by a need to fulfill public desires. The difference is that it is now data-driven. This sounds dystopian, but it’s not a bad thing - many great TV shows have come out of this, and wouldn’t exist without it.</p>
<p>We’ll have to wait to see how this evolves. I like to think of AI as augmenting the creator, versus taking my job away, but maybe this is just wishful thinking.</p>
<!--
# Thoughts

Any tools like the memex? 
-->

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>NIPS 2017 Deep learning panel</title>
    <link href="http://holdenlee.github.io/blog/posts/cs/ml/nips_2017.html" />
    <id>http://holdenlee.github.io/blog/posts/cs/ml/nips_2017.html</id>
    <published>2017-12-22T00:00:00Z</published>
    <updated>2017-12-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>NIPS 2017 Deep learning panel</h1>
    </div>
    
    <div class="info">
       <div class="subtitle"><p>Summary</p></div> 
       
        <p>Posted: 2017-12-22 
          , Modified: 2017-12-22 
	</p>
      
       <p>Tags: <a href="/tags/neural%20network.html">neural network</a>, <a href="/tags/deep%20learning.html">deep learning</a>, <a href="/tags/machine%20learning.html">machine learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#sa-name-one-aspect-of-dl-where-you-think-theory-can-make-a-deep-impact.">SA: Name one aspect of DL where you think theory can make a deep impact.</a></li>
 <li><a href="#sa-is-the-presence-of-adversarial-examples-simply-a-puzzle-or-is-it-a-fundamental-problem-which-will-give-deep-insight">SA: Is the presence of adversarial examples simply a puzzle, or is it a fundamental problem which will give deep insight?</a></li>
 <li><a href="#sa-is-generative-models-the-correct-approach-to-unsupervised-learning">SA: Is generative models the correct approach to unsupervised learning?</a></li>
 <li><a href="#sa-does-rl-need-deep-learning-other-than-to-sense-the-environment">SA: Does RL need deep learning other than to sense the environment?</a></li>
 <li><a href="#sa-generalization---is-it-just-a-puzzle-or-does-it-lead-to-fundamental-new-insights">SA: Generalization - is it just a puzzle or does it lead to fundamental new insights?</a></li>
 <li><a href="#a-do-we-need-to-go-beyond-first-order-methods">A: Do we need to go beyond first order methods?</a></li>
 <li><a href="#a-can-we-use-generative-models-to-prevent-adversarial-examples">A: Can we use generative models to prevent adversarial examples?</a></li>
 <li><a href="#sa-what-about-logic">SA: What about logic?</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>This is a summary of the panel discussion at the NIPS 2017 <a href="https://ludwigschmidt.github.io/nips17-dl-workshop-website/">deep learning (bridging theory and practice) workshop</a>.</p>
<p>I’ve paraphrased the conversation; any misrepresentations are due to me.</p>
<p>Host: Sanjeev Arora (SA)</p>
<p>Panelists:</p>
<ul>
<li>Ruslan Salakhutdinov (RS)</li>
<li>Percy Liang (PL)</li>
<li>Yoshua Bengio (YB)</li>
<li>Peter Bartlett (PB)</li>
<li>Sham Kakade (SK)</li>
</ul>
<p>Audience (A)</p>
<h2 id="sa-name-one-aspect-of-dl-where-you-think-theory-can-make-a-deep-impact.">SA: Name one aspect of DL where you think theory can make a deep impact.</h2>
<p>SK: Model-based solution concepts.</p>
<p>PL: Inductive bias. This is important in language understanding. Right now, model creators are mostly guided by intuition. Attention mechanisms are an example of inductive bias.</p>
<p>PB: Optimization and statistical properties, and generalization. Even though models are hugely overparametrized, the optimization leads to a solution that generalizes.</p>
<p>YB: In the past, neural networks have been in the realm of theory. But now, people doing theory should look in many other places as well. For example, to figure out how to formalize questions about uniqueness of representations, we can investigate when we get unique solutions to nonlinear ICA.</p>
<h2 id="sa-is-the-presence-of-adversarial-examples-simply-a-puzzle-or-is-it-a-fundamental-problem-which-will-give-deep-insight">SA: Is the presence of adversarial examples simply a puzzle, or is it a fundamental problem which will give deep insight?</h2>
<p>SK: This is important in reinforcement learning (RL) because in some sense errors in RL always look like worst-case errors. Making progress on robustness in RL will be like making progress on adversarial learning. We don’t understand how error in RL compounds.</p>
<p>PL: When I first learned about adversarial examples I thought it was cute, but over the years I found that it led to many other ideas. It relates to interpretability, teaching, generalizing models to new distributions, robustness (is it learning the right model?), and having the right inductive bias. Adversarial examples good way to stress-test a model.</p>
<p>PB: Defending against adversarial attacks is important. Though, adversarial examples may be a phenomena of many systems (even humans).</p>
<p>YB: It’s not specific to neural nets, but a broader problem. My intuition from the beginning is that it has a lot to do with the fact that humans use a lot of extra information about the data distribution <span class="math inline">\(p(x)\)</span>, and this influences our classifiers in a strong way - we mostly learn <span class="math inline">\(p(x,y)\)</span>, not <span class="math inline">\(p(y|x)\)</span>. Attempts to jointly learn the input probability distribution and the classifier were not very convincing, so this early idea was forgotten.</p>
<p>RS: Building good generative models would help. But they are hard to build.</p>
<p>YB: If we capture 3-D models of images, a small perturbation won’t disturb them.</p>
<p>PL: Wouldn’t they? You can still attack the generative model.</p>
<h2 id="sa-is-generative-models-the-correct-approach-to-unsupervised-learning">SA: Is generative models the correct approach to unsupervised learning?</h2>
<p>SK: Even if density estimation can do extremely well, what we care about is the last bit of error. The generative model can be accurate in most natural senses, yet the model can still fail to get fill-in-the-blank questions like “Jane is going to work. Her car broke down. She was _ (late).” wrong most of the time. It won’t do well until it completely learns the distribution.</p>
<p>PL: How do we set up the objective functions to explain the high-level information and semantics?</p>
<p>PB: A central issue is, what’s the appropriate objective function? We can use user interaction to pin it down.</p>
<p>YB: Consider: if an algorithm models acoustics with an information-theoretical objective, it has hard time model speech, because that is only a few bits of relevant abstract information out of thousand bits of signal. Instead of considering the problem as a generative problem (with objectives like reconstruction error, entropy), look instead in the representation space: which directions matter? (See <a href="https://arxiv.org/abs/1709.08568">consciousness prior</a>.)</p>
<p>RS: I believe in generative models that try to learn about environment by interacting with it.</p>
<p>YB: Don’t predict in pixel space - predict in the abstract space.</p>
<p>SK: I believe in generative models for language. there, but we’re still missing concepts.</p>
<p>RS: 7-8 years ago, people thought dealing with pixels is a disaster, so they made features using HOG and SIFT. But deep nets work directly on pixels! The input is pixels, but deep nets go from that to a higher-level space.</p>
<p>PL: Learning to generate text isn’t the same as coming up with a generative model for text. We don’t to cover the space; RL can just go to a mode.</p>
<h2 id="sa-does-rl-need-deep-learning-other-than-to-sense-the-environment">SA: Does RL need deep learning other than to sense the environment?</h2>
<p>SK: In the vanilla control setting, I’m not convinced it does. For harder questions, I’m still not convinced current methods are doing anything more that just building a bridge between ways to represent the environment, and sequential decision-making solution. Harder questions involve logic and planning - is there currently an impressive demo where we’re seeing that? I think the deep learning part is just giving better representations of the environment.</p>
<p>PL: In the long term, RL will need sensing, the ability to hold large memory about the world, do reasoning, and express a complicated control policy.</p>
<p>PB: It would be surprising if you could solve complex control problems aside from sensing without having rich nonparametric classes - if there are not additional barriers. In most current examples you can get by with linear systems.</p>
<p>RS: Deep learning is helpful here because it learns the right representation for the task; it goes from the raw sensing to the representation and the policy, and you can just backprop. There are more things we can do beyond extracting features: have separate architecture for storing information, attention, etc.</p>
<p>PL: We need to have baselines. There are cases (in dialog and in control) where simple nondeep models with the right inductive bias do better than everything with a deep neural net. We need to carefully measure where the value of deep learning is coming in.</p>
<h2 id="sa-generalization---is-it-just-a-puzzle-or-does-it-lead-to-fundamental-new-insights">SA: Generalization - is it just a puzzle or does it lead to fundamental new insights?</h2>
<p>On a practical level, we can just hold out some data for testing.</p>
<p>PB: There is more to understand. Most interesting is the connection between optimization and generalization. At the qualitative level, we understand a lot of measures of complexity. What are the implications for regularization? Why is SGD effective in finding solutions that generalize well? When we get that insight, we will find better algorithms, in particular for very noisy cases.</p>
<p>PL: With a practitioner’s hat, I ask: if I understand the generalization/optimization story how would that change my behavior? I care about choosing families of functions with good inductive biases (that will make me have low approximation error) - this guides the first thing I do when I’m going into a new domain, and figuring out what works. Is there a more end-to-end way of understanding approximation of functions and generalization at same time, something that allows jointly optimizing them?</p>
<p>RS: It’s hard to beat the bag of tricks (SGD + dropout + batch normalization + momentum).</p>
<p>SK: The question is a proxy for thinking about algorithms which generalize. If all we care about is verification, just hold out data. We hope that the theory of generalization can help. On a fixed problem hard to beat SGD with dropout.</p>
<p>But there’s a richer set of questions from thinking about generalization more broadly: generalization between problems (transfer learning) multitask learning, domain adaptation - all frustrations of ML.</p>
<p>PL: If you train on sequences of length 5, can you generalize to length 10? Images of different size? Extrapolation is important, and necessitates learning the right concepts.</p>
<p>MR: Does a neural net learn sensible abstractions and high-level concepts? A cognitive neuroscience perspective may help. How do we rethink architecture (e.g. Jeff Hinton’s capsules)?</p>
<p>RS: Design new architectures with notions of invariance - this adds in prior knowledge. A CNN learns things that a non-convolutional net finds hard; a CNN has lots of prior knowledge!</p>
<p>Given a face with 3 eyes, a neural network still thinks it’s face. How do we incorporate priors? Memory is exciting (e.g., NTMs can write and read). It’s not clear how well they will work. How to design new architectures with inductive biases?</p>
<p>PL: Inductive biases are important for improving sample efficiency. It’s more necessary to learn the right thing. For images, classifiers aren’t robust to changing the fourier spectrum of the images.</p>
<p>I feel pessimistic that an architecture can magically learn the right thing, and have concepts that generalize well. But in practice, we can get good performance and useful products.</p>
<h2 id="a-do-we-need-to-go-beyond-first-order-methods">A: Do we need to go beyond first order methods?</h2>
<p>SK: In RL, a lot of solution concepts aren’t just first order. Many use trust region methods. How does Monte Carlo tree search fit in? There’s issues of discrete vs. continuous optimization</p>
<p>RS: It’s hard to beat SGD+momentum+BN. But there should be something going beyond it.</p>
<h2 id="a-can-we-use-generative-models-to-prevent-adversarial-examples">A: Can we use generative models to prevent adversarial examples?</h2>
<p>PL: Text adversarial examples cannot be detected by generative models. For adversarial examples, we need to think beyond the statistical worst case notion; <span class="math inline">\(L^\iy\)</span> is the tip of the iceberg.</p>
<p>SK: Theory has a valuable contribution to make, on robustness (to adversarial perturbation).</p>
<p>PL: Argmax is hard.</p>
<p>SA: There’s something off about an information theoretic approach to unsupervised learning. When I imagine a scene with people, it’s not clear that it’s coming out of a distribution.</p>
<p>PL: To generate language, if suffices to havea policy. Individual humans don’t have distribution over sentences. We all have different distributions, can still get high rewards.</p>
<p>SA: Is there a distribution?</p>
<p>PB: This is like the determinism vs. free will question… You generate sentences in particular way. There’s a distribution; I don’t see that as a problem.</p>
<p>SA: This could be a philosophical or a practical issue. On the practical side, the most important thing could be the last bit which is not learned.</p>
<p>PL: How do we fit language? Machine learning often learns generic responses, which not interesting. We could flip the KL, maximize expectation with respect to policy as opposed to the distribution. We don’t have to cover the space, just find one solution out of model, which is easier than modeling the whole distribution.</p>
<h2 id="sa-what-about-logic">SA: What about logic?</h2>
<p>SA: My favorite research problem is to combine differentiable techniques with what introspection tells us - old AI logic. Can we come up with differentiable reasoning?</p>
<p>RS: What about fuzzy logic?</p>
<p>PL: I work a lot on semantic parsing. Logic allows you to move big pieces of things. Taking the maximum works reliably. Get extrapolation for free.</p>
<p>On the other hand, logic is a straitjacket, and has sharp cliffs. <!--Given a trained neural net can we coerce it to logic?--></p>
<p>We can use logic by pragmatically encoding the primitives and having a good inductive bias; having a logical backbone is like growing a vine using trellises.</p>
<p>RS: How do we combine logical rules with deep learning, so that if the logical rules make sense then the model picks them up, and if not, it gets rid of them?</p>
<p>For David Silver’s work on Go, he noted that Go players had specific rules. The neural network discovered some of them, and kept the important ones.</p>
<p>We can put logical rules into the prior, but then the net should figure out what makes sense.</p>
<p>SK: In RL, planning seems logic-based. Alpha-go is a interesting example: it leans on MCTS heavily.</p>
<p>Here deep learning is fitting a continuous approximation of the world.</p>
<p>PL: I want to decouple the idea of logic as a representation vs. as a replacement for learning.</p>
<p>It’s more useful to think of it as a representation: certain parts of a problem have structure. Think of logic as making things digital vs. analog. For long-horizon situations, if you want to not forget, do error correction, and logic is doing something similar.</p>
<p>SA: Will there be grand synthesis of deep learning and 60’s/70’s (good-old-fashioned) AI?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Crash Course in Complex Analysis</title>
    <link href="http://holdenlee.github.io/blog/posts/math/analysis/complex_cs.html" />
    <id>http://holdenlee.github.io/blog/posts/math/analysis/complex_cs.html</id>
    <published>2017-08-02T00:00:00Z</published>
    <updated>2017-08-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Crash Course in Complex Analysis</h1>
    </div>
    
    <div class="info">
       <div class="subtitle"><p>geared towards computer scientists</p></div> 
       
        <p>Posted: 2017-08-02 
          , Modified: 2017-08-02 
	</p>
      
       <p>Tags: <a href="/tags/complex%20analysis.html">complex analysis</a>, <a href="/tags/crash%20course.html">crash course</a>, <a href="/tags/numerical%20analysis.html">numerical analysis</a>, <a href="/tags/numerical%20integration.html">numerical integration</a>, <a href="/tags/polynomials.html">polynomials</a>, <a href="/tags/Chebyshev%20polynomials.html">Chebyshev polynomials</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Here are notes for my talk “Crash course in complex analysis” for the Gems of TCS seminar.</p>
<ul>
<li>For a crash course, see the first chapter in my notes on analytic number theory, <a href="http://tiny.cc/annt" class="uri">http://tiny.cc/annt</a>. Proof sketches are in the <a href="https://www.dropbox.com/s/k2qf4oy754ln8og/complex_handwritten_notes.pdf?dl=0">handwritten notes</a>.</li>
<li>For applications, see notes <a href="https://www.dropbox.com/s/fn854rj0j98ij6d/complex_cs.pdf?dl=0">here</a>.</li>
</ul>
<p><strong>Abstract</strong>: Calculus on the complex numbers has an entirely different attitude from calculus on the reals. The additional structure of the complex numbers greatly enriches the theory of differentiable functions. I will cover (the many variants of) Cauchy’s Theorem, power (Laurent) series, and various inequalities (Hadamard three-lines and three-circles). I’ll give some applications including convergence rates for polynomial approximations and numerical integration, and a recent application to population recovery.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>On the ability of neural nets to express distributions (at COLT 2017)</title>
    <link href="http://holdenlee.github.io/blog/posts/cs/ml/neural_net_distributions.html" />
    <id>http://holdenlee.github.io/blog/posts/cs/ml/neural_net_distributions.html</id>
    <published>2017-07-07T00:00:00Z</published>
    <updated>2017-07-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>On the ability of neural nets to express distributions (at COLT 2017)</h1>
    </div>
    
    <div class="info">
       <div class="subtitle"><p>Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, Sanjeev Arora</p></div> 
       
        <p>Posted: 2017-07-07 
          , Modified: 2017-07-07 
	</p>
      
       <p>Tags: <a href="/tags/machine%20learning.html">machine learning</a>, <a href="/tags/paper.html">paper</a>, <a href="/tags/neural%20network.html">neural network</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#abstract">Abstract</a></li>
 <li><a href="#paper">Paper</a></li>
 <li><a href="#poster">Poster</a></li>
 <li><a href="#slides">Slides</a></li>
 <li><a href="#background">Background</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Shortlink: <a href="http://tiny.cc/hlcolt17" class="uri">http://tiny.cc/hlcolt17</a></p>
<h2 id="abstract">Abstract</h2>
<p>Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution–also theoretically not understood–concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks.</p>
<p>We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with <span class="math inline">\(n\)</span> hidden layers. A key ingredient is Barron’s Theorem [Barron1993], which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of n functions which satisfy certain Fourier conditions (“Barron functions”) can be approximated by a <span class="math inline">\(n+1\)</span>-layer neural network.</p>
<p>For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance–a natural metric on probability distributions–by a neural network applied to a fixed base distribution (e.g., multivariate gaussian). Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone.</p>
<h2 id="paper">Paper</h2>
<ul>
<li><a href="https://arxiv.org/abs/1702.07028">Arxiv page</a></li>
<li><a href="https://arxiv.org/pdf/1702.07028">PDF</a></li>
</ul>
<h2 id="poster">Poster</h2>
<ul>
<li><a href="https://www.dropbox.com/s/rjkc0u6estet9sz/barron_poster.pptx?dl=0">Powerpoint</a></li>
<li><a href="https://www.dropbox.com/s/4niwh4hkro6n882/barron_poster.pdf?dl=0">PDF</a></li>
</ul>
<h2 id="slides">Slides</h2>
<ul>
<li><a href="https://www.dropbox.com/s/lioa0e7wc8s8g3e/barron_presentation.pdf?dl=0">Slides</a></li>
<li><a href="https://workflowy.com/s/wL3CyEXpHY#/e7b2a09e6a24">Transcript</a></li>
</ul>
<h2 id="background">Background</h2>
<p><a href="https://www.dropbox.com/s/4du02wdl58aaghk/barron1.pdf?dl=0">Expository note</a> on Barron’s Theorem.</p>
<p>Thoughts, questions, typos? Leave a comment below.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Thoughts and links, 3-4/2017</title>
    <link href="http://holdenlee.github.io/blog/posts/monthly_summaries/2017-4.html" />
    <id>http://holdenlee.github.io/blog/posts/monthly_summaries/2017-4.html</id>
    <published>2017-05-01T00:00:00Z</published>
    <updated>2017-05-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Thoughts and links, 3-4/2017</h1>
    </div>
    
    <div class="info">
      
       
        <p>Posted: 2017-05-01 
          , Modified: 2017-05-01 
	</p>
      
       <p>Tags: <a href="/tags/links.html">links</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>See all the links for <a href="https://workflowy.com/s/wL3CyEXpHY#/d1a52d188a8f?q=%232017-3">March</a> and <a href="https://workflowy.com/s/wL3CyEXpHY#/d1a52d188a8f?q=%232017-4">April</a>. (See <a href="4-16.html">April 2016’s post</a> for an explanation of the organization.) Here are some highlights.</p>
<h2 id="books">Books</h2>
<ul>
<li><a href="https://www.goodreads.com/book/show/28202.The_Metaphysical_Club">The Metaphysical Club, Louis Menand</a>: Discusses the evolution of American philosophy in the 19th and early 20th century in response to the Civil War and scientific progress (ex. Darwin’s theory of evolution), through the story of 4 American philosphers (William James, Oliver Wendell Holmes, Charles Peirce, and John Dewey). <a href="https://workflowy.com/s/4qkO9xWz4M#/9ccd29dd9117">Notes</a></li>
<li><a href="https://www.goodreads.com/book/show/10165606-introduction-to-classical-chinese-philosophy">Introduction to classical Chinese philosophy, Bryan Van Norden</a>, <a href="https://workflowy.com/s/wL3CyEXpHY#/45fec88080b3">notes</a></li>
<li><a href="http://www.rifters.com/real/Blindsight.htm">Blindsight, Peter Watts</a> asks the question: what if the correlation of consciousness and intelligence on Earth is just an accident - and the universe is filled with intelligent but nonconscious beings? It makes a strong (scientific, non-dualistic) argument for the possibility that intelligence need not be correlated with consciousness.</li>
<li><a href="https://www.goodreads.com/book/show/3896708-china">China: A History, John Keay</a>: in progress, <a href="https://workflowy.com/s/wL3CyEXpHY#/0ad51ae14b7f">notes</a></li>
</ul>
<h2 id="comics">Comics</h2>
<p>A free trial of <a href="https://www.comixology.com/">Comixology</a> was very much worth it. Some of my picks:</p>
<ul>
<li><a href="https://www.comixology.com/Trees-Vol-1/digital-comic/194144?ref=c2VhcmNoL2luZGV4L2Rlc2t0b3Avc2xpZGVyTGlzdC90b3BSZXN1bHRzU2xpZGVy">Trees</a></li>
<li><a href="https://www.comixology.com/reMIND-Vol-1/digital-comic/197610?ref=c2VhcmNoL2luZGV4L2Rlc2t0b3Avc2xpZGVyTGlzdC90b3BSZXN1bHRzU2xpZGVy">reMIND</a></li>
<li><a href="https://www.comixology.com/Adventure-Time-Vol-1/digital-comic/47066?ref=c2VhcmNoL2luZGV4L2Rlc2t0b3Avc2xpZGVyTGlzdC90b3BSZXN1bHRzU2xpZGVy">Adventure time</a>
<ul>
<li><a href="https://www.comixology.com/Adventure-Time-Fionna-Cake/comics-series/9458?ref=c2VhcmNoL2luZGV4L2Rlc2t0b3Avc2xpZGVyTGlzdC9zZXJpZXNTbGlkZXI">Fiona and Cake</a></li>
</ul></li>
<li><a href="https://www.comixology.com/Pretty-Deadly/comics-series/11683?ref=c2l0ZS9saXN0L2Rlc2t0b3AvbGlzdC9HZW5yZUxpc3Q">Pretty deadly</a></li>
</ul>
<h2 id="podcast-picks">Podcast Picks</h2>
<ul>
<li>The Allusionist
<ul>
<li><a href="http://www.theallusionist.org/allusionist/please">33 Please</a>: Differences in the usage of “please” in the US and UK.</li>
<li>50-51 Under the Covers <a href="http://www.theallusionist.org/allusionist/covers-i">1</a> and <a href="http://www.theallusionist.org/allusionist/covers-ii">2</a>: On romance novels and the language of sex.</li>
<li><a href="https://www.theallusionist.org/allusionist/authority">54 The Authority</a>: The role of the dictionary.</li>
</ul></li>
<li><a href="http://www.radiolab.org/story/radiolab-extra-henrietta-lacks/">Radiolab Extra: Henrietta Lacks</a>: On one woman’s medically miraculous cancer cells.</li>
<li>Imaginary Worlds: <a href="https://soundcloud.com/emolinsky/beyond-the-iron-curtain">Beyond the Iron Curtain</a>: Science fiction from the Soviet Union tends to be more subtle.</li>
<li>The Memory Palace: <a href="http://thememorypalace.us/2017/02/amok/">105, Amok</a>: when animals escape the Central Park Zoo.</li>
<li>On being: <a href="https://onbeing.org/https://onbeing.org/programs/richard-rohr-living-in-deep-time/">Richard Rohr — Living in Deep Time</a> (<a href="http://scrible.com/s/22FSA">highlights</a>): “Chronos is chronological time, time as duration, one moment after another, and that’s what most of us think of as time… But there was another word in Greek, kairos. And kairos was deep time. It was when you have those moments where you say, ‘Oh my god, this is it.’”</li>
<li>Philosophy Bites: <a href="http://philosophybites.com/2017/03/andy-clark-on-the-extended-mind.html">Andy Clark on The Extended Mind</a></li>
<li>Quanta: <a href="https://www.quantamagazine.org/20170202-math-and-the-best-life-francis-su-interview/">To live your best life, do mathematics</a>: Interview with Francis Su.</li>
<li>TED radio hour
<ul>
<li><a href="http://www.npr.org/programs/ted-radio-hour/514152888/the-spirit-of-inquiry?showDate=2017-02-24">The spirit of inquiry</a></li>
<li><a href="http://www.npr.org/programs/ted-radio-hour/519264798/decisions-decisions-decisions?showDate=2017-03-10">Decisions</a></li>
</ul></li>
<li>The Truth
<ul>
<li><a href="http://www.thetruthpodcast.com/story/2017/3/8/miracle-on-the-l-train">Miracle on the L train</a></li>
</ul></li>
<li>This American Life:
<ul>
<li><a href="https://www.thisamericanlife.org/radio-archives/episode/611/vague-and-confused">611 Vague and confused</a> including a story on who gets to stay on a private Hawaiian island.</li>
<li><a href="https://www.thisamericanlife.org/radio-archives/episode/614/the-other-mr-president">614 The other Mr. President</a> (Vladimir Putin)</li>
</ul></li>
<li>99 percent invisible
<ul>
<li><a href="http://99percentinvisible.org/episode/state-sanctuary-part-2/">250 State (Sanctuary Part 2)</a>: The role of churches as sanctuary in the refugee crisis.</li>
<li><a href="http://99percentinvisible.org/episode/containers-ships-tugs-port/">254 Containers</a>: The change brought by the invention of containers</li>
</ul></li>
</ul>
<h2 id="articles">Articles</h2>
<ul>
<li>Map and territory:
<ul>
<li><a href="https://mapandterritory.org/need-dynamics-54ca9ff5955c">Need dynamics</a>: “Coference” seems like a useful concept.</li>
<li><a href="https://mapandterritory.org/narrativemancy-101-why-paper-beats-rock-bc25bc1147b6">Narrativemancy 101</a>: The power of narrative comes from how it references your most vivid feelings.</li>
</ul></li>
<li>Slatestarcodex: <a href="https://slatestarcodex.com/2015/03/25/is-everything-a-religion/">Is everything a religion?</a></li>
<li><a href="http://lesswrong.com/lw/gej/i_attempted_the_ai_box_experiment_and_lost/">The AI box experiment</a></li>
<li><a href="https://blog.openai.com/evolution-strategies/">Evolutionary strategies</a> as an alternative to reinforcement learning (<a href="http://scrible.com/s/00V4A">h</a>)</li>
</ul>
<h2 id="an-amazing-piece-of-my-little-pony-fanfiction">An amazing piece of My Little Pony Fanfiction</h2>
<p><a href="http://www.fimfiction.net/story/62074/friendship-is-optimal">Friendship is optimal</a></p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Thoughts and links, 1-2/2017</title>
    <link href="http://holdenlee.github.io/blog/posts/monthly_summaries/2017-2.html" />
    <id>http://holdenlee.github.io/blog/posts/monthly_summaries/2017-2.html</id>
    <published>2017-03-05T00:00:00Z</published>
    <updated>2017-03-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Thoughts and links, 1-2/2017</h1>
    </div>
    
    <div class="info">
      
       
        <p>Posted: 2017-03-05 
          , Modified: 2017-03-05 
	</p>
      
       <p>Tags: <a href="/tags/links.html">links</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>See all the links for <a href="https://workflowy.com/s/wL3CyEXpHY#/d1a52d188a8f?q=%232017-1">January</a> and <a href="https://workflowy.com/s/wL3CyEXpHY#/d1a52d188a8f?q=%232017-2">February</a>. (See <a href="4-16.html">April’s post</a> for an explanation of the organization.) Here are some highlights.</p>
<h2 id="podcast-picks">Podcast picks</h2>
<ul>
<li>Imaginary worlds
<ul>
<li><a href="https://soundcloud.com/emolinsky/28-days-of-black-cosplay">28 days of black cosplay</a></li>
<li><a href="https://soundcloud.com/emolinsky/growing-up-avatar-american">Growing up Avatar-American</a>: Why is Avatar so popular among Asian-Americans?</li>
</ul></li>
<li>On being
<ul>
<li><a href="http://onbeing.org/programs/alain-de-botton-the-true-hard-work-of-love-and-relationships/">Alain de Botton - The true hard work of love and relationships</a> (<a href="https://workflowy.com/s/wL3CyEXpHY#/0d15c4e47c78">notes</a>)</li>
<li><a href="http://www.onbeing.org/programs/maria-popova-cartographer-meaning-digital-age/">Maria Popova - Cartographer of meaning in a digital age</a> (<a href="https://workflowy.com/s/wL3CyEXpHY#/e3859eac9b59">notes</a>)</li>
</ul></li>
<li>Radiolab
<ul>
<li><a href="http://www.radiolab.org/story/stanger-paradise/">Stranger in paradise</a> Raccoons as an endangered species, or not?</li>
<li><a href="http://www.radiolab.org/story/radiolab-presents-media-busted-americas-poverty-myths/">On the media: busted, America’s poverty myths</a>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> See also <a href="http://freakonomics.com/podcast/american-dream-really-dead/">Freakonomics: Is the American Dream really dead?</a></li>
</ul></li>
<li>Rationally speaking
<ul>
<li><a href="http://rationallyspeakingpodcast.org/show/rs-176-jason-brennan-on-against-democracy.html">176 - Against Democracy</a> What if democracy is not the best system of government?</li>
<li><a href="http://static1.1.sqspcdn.com/static/f/468275/27460112/1487645326207/rs178transcript.pdf?token=awXHfsGHawhCx3qAlBjpX7HiISU%3D">178 - Tim Urban on “Trying to live well, as semi-rational animals”</a></li>
</ul></li>
<li>Reply All
<ul>
<li><a href="https://gimletmedia.com/episode/86-man-of-the-people/">86 Man of the people</a> A mindblowing story about how far a scammer got by harnessing the power of a new technology - the radio.</li>
<li><a href="https://gimletmedia.com/episode/87-longmont/">87 Storming the castle</a> Prank calls as an art form, by <a href="http://longmontpotioncastle.com/">Longmont potion castle</a></li>
</ul></li>
<li>Reveal (investigative reporting)
<ul>
<li><a href="https://www.revealnews.org/episodes/the-year-in-reveal/">The year in Reveal</a>, including an interview with white nationalist Richard Spencer</li>
<li><a href="https://www.revealnews.org/episodes/the-man-inside-four-months-as-a-prison-guard/">The man inside: four months as a prison guard</a>: inside private prisons</li>
</ul></li>
<li>The Truth: <a href="http://www.thetruthpodcast.com/story/2017/1/11/the-dark-end-of-the-mall">The dark end of the mall</a> A post-apocalytic story</li>
<li>This American Life
<ul>
<li><a href="https://www.thisamericanlife.org/radio-archives/episode/607/didn%E2%80%99t-we-solve-this-one">607 Didn’t we solve this one?</a> Iraqis risk their lives by helping American forces, but get rejected from immigration.</li>
<li><a href="https://www.thisamericanlife.org/radio-archives/episode/610/grand-gesture">610 Grand gestures</a></li>
</ul></li>
<li>99 percent invisible
<ul>
<li><a href="http://99percentinvisible.org/episode/the-revolutionary-post/">244 The revolutionary post</a> - history of the postal service</li>
<li>On Frank Lloyd Wright’s architecture: <a href="http://99percentinvisible.org/episode/usonia-1/">246 Usonia 1</a> and <a href="http://99percentinvisible.org/episode/usonia-the-beautiful/">247 Usonia the beautiful</a></li>
<li><a href="http://99percentinvisible.org/episode/atom-garden-eden/">248 Atom in the garden of Eden</a> - the dream of the nuclear age, manifested in “gamma gardens”</li>
</ul></li>
<li>青春愛消遣
<ul>
<li><a href="http://youngloveplay.blogspot.com/2017/02/056.html">56 愛上德國男</a>: A story about love and limerence.</li>
</ul></li>
</ul>
<h2 id="articles-and-websites">Articles and websites</h2>
<ul>
<li><a href="http://zenpencils.com/">Zenpencils</a>: inspirational cartoon quotes</li>
<li>Rationality
<ul>
<li>Otium
<ul>
<li><a href="https://srconstantin.wordpress.com/2016/10/20/ra/">Ra</a> <a href="http://scrible.com/s/ifMm6">h</a>: A very good articulation of the corrupting force of generic superlativity. Related to Moloch, etc.</li>
<li><a href="https://srconstantin.wordpress.com/2016/03/21/there-is-no-secret-notebook/">There is no secret notebook</a> <a href="http://scrible.com/s/gDwm6">h</a></li>
</ul></li>
<li>Satvik Beri
<ul>
<li><a href="http://satvikberi.com/2016/12/26/increasing-output/">Increasing output</a>: Advice both for personal productivity and management.</li>
<li><a href="http://satvikberi.com/2015/03/31/efficiency-vs-disproportionate-results/">Efficiency vs. disproportionate results</a> <a href="http://scrible.com/s/kLx2C">h</a>: Two modes for solving problems that are too often disjoint.</li>
</ul></li>
</ul></li>
<li><a href="https://www.cs.princeton.edu/news/crowd-wisdom-surprisingly-popular-answer-can-trump-ignorance-masses">“Surprisingly popular” answer can trump ignorance of the masses</a></li>
<li><a href="https://airtable.com/">Airtable</a>: Useful spreadsheet program which can save multiple sort/filter views</li>
<li>AI safety resources:
<ul>
<li><a href="http://humancompatible.ai/bibliography#corrigibility">Human-compatible AI bibliography</a></li>
<li><a href="https://agentfoundations.org/">Agent foundations</a> (the “forum digest” posts are very helpful)</li>
<li><a href="https://arxiv.org/abs/1606.06565">Concrete problems in AI safety</a></li>
<li><a href="https://intelligence.org/2016/07/27/alignment-machine-learning/">Alignment in machine learning</a></li>
</ul></li>
</ul>
<h2 id="books">Books</h2>
<ul>
<li><a href="https://www.goodreads.com/book/show/332613.One_Flew_Over_the_Cuckoo_s_Nest">One flew over the cuckoo’s nest</a> - Set in a psychiatric hospital, but manages to a hopeful and humorous (and sad) story.</li>
<li><a href="https://www.goodreads.com/book/show/7405989-the-dragon-and-the-stars">The dragon and the stars</a> - collection of Chinese-themed sci-fi/fantasy short stories</li>
<li>Farseer trilogy, Robin Hobb - follows the life of an assassin, with all its moral dilemmas. I liked the treatment of magic (the Skill - telepathic ability to talk with or influence other people - and the Wit - a communing with animals).
<ul>
<li><a href="https://www.goodreads.com/book/show/77197.Assassin_s_Apprentice">Assassin’s Apprentice, Robin Hobb</a></li>
<li><a href="https://www.goodreads.com/book/show/25300956-royal-assassin">Royal Assassin, Robin Hobb</a></li>
</ul></li>
<li><a href="https://www.goodreads.com/book/show/18937.In_the_Beginning_Was_the_Command_Line">In the beginning was the command line, Neal Stephenson</a> - I wished I’d read this a long time ago; it illuminated some basic knowledge about computers which I lacked. A <a href="https://www.learnenough.com/command-line-tutorial">command line tutorial</a>.</li>
<li><a href="https://www.goodreads.com/book/show/22414.The_Invisible_Heart">The invisible heart: An economic romance, Russell Roberts</a> - A conversation about economics delightfully embedded in a love story. Made some good arguments for deregulation.</li>
<li><a href="https://www.goodreads.com/book/show/156785.Diaspora">Diaspora, Greg Egan</a> - Greg Egan goes very far in imagining the evolution of minds in cyberspace. Some interesting metaphors for mathematics. Different ways of embedding life/computation in the universe. A journey through multiple universes in search for…?</li>
<li>Comics!
<ul>
<li><a href="https://www.goodreads.com/book/show/25546167-descender-vol-1">Descender 1</a> After a mysterious AI wreaks destruction, robots are outlawed. Follows the story of an android who wakes up in this world.</li>
<li><a href="https://www.goodreads.com/book/show/15704307-saga-vol-1">Saga 1</a> A couple from opposite sides of a galactic war tries to escape being hunted on all sides. Writing and art is fantastic. Also, +1 for Esperanto.</li>
</ul></li>
<li><a href="https://www.goodreads.com/book/show/22571552-so-you-ve-been-publicly-shamed">So you’ve been publicly shamed, Jon Ronson</a>. Starting with several stories of Twitter shaming gone out of control, investigates how public shaming has transformed in recent years, the morality and role of public shaming, how to mitigate it, etc.</li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>A quote: when we feel gratitude we’re more generous to strangers, when we’re more aware of luck’s importance, we’re more likely to plow our own fortune back into the common good. But we underplay luck because we can recall our own struggles far better than the fateful but fuzzy role of chance and because the very idea corrodes our faith in free will, but mostly because like Benny Franklin, we’re deeply invested in our own autobiographies.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Interactive Learning (Simons Workshop)</title>
    <link href="http://holdenlee.github.io/blog/posts/cs/ml/simons_ml_interactive.html" />
    <id>http://holdenlee.github.io/blog/posts/cs/ml/simons_ml_interactive.html</id>
    <published>2017-02-13T00:00:00Z</published>
    <updated>2017-02-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Interactive Learning (Simons Workshop)</h1>
    </div>
    
    <div class="info">
       <div class="subtitle"><p>Notes</p></div> 
       
        <p>Posted: 2017-02-13 
          , Modified: 2017-02-13 
	</p>
      
       <p>Tags: <a href="/tags/machine%20learning.html">machine learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>I’m currently attending the <a href="https://simons.berkeley.edu/workshops/machinelearning2017-1">Interactive Learning</a> workshop at the Simons Institute.</p>
<p>Here are my <a href="https://www.dropbox.com/s/mxbbdv8i11f63an/interactive_workshop.pdf?dl=0">notes</a>. Source is available on <a href="https://github.com/holdenlee/simons-ml">github</a>.</p>
<p>I’m also taking notes on seminars and reading groups at the Simons Program in Machine Learning. All notes are available on the <a href="http://simons.squidhive.net/Notes">Simons Program wiki</a>.</p>
<p>I will update this page as the workshop progresses.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Goodnight Mel</title>
    <link href="http://holdenlee.github.io/blog/posts/writing/goodnight_mel.html" />
    <id>http://holdenlee.github.io/blog/posts/writing/goodnight_mel.html</id>
    <published>2017-01-29T00:00:00Z</published>
    <updated>2017-01-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Goodnight Mel</h1>
    </div>
    
    <div class="info">
       <div class="subtitle"><p>A ghost story</p></div> 
       
        <p>Posted: 2017-01-29 
          , Modified: 2017-01-29 
	</p>
      
       <p>Tags: <a href="/tags/fiction.html">fiction</a>, <a href="/tags/short%20story.html">short story</a>, <a href="/tags/ghost%20story.html">ghost story</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>I’m proud to present my newest short story, Goodnight Mel, about a couple who moves into a house haunted by a girl who plays strange music.</p>
<p>You can read it <a href="https://docs.google.com/document/d/1Pxs_3NstJrN3YwTQBmLDaLNwBJv6FBGzF23zDpFY0UI/edit">here</a>.</p>
<p>Feedback is welcome!</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Foundations of Machine Learning Boot Camp (Simons Workshop)</title>
    <link href="http://holdenlee.github.io/blog/posts/cs/ml/simons_ml_bootcamp.html" />
    <id>http://holdenlee.github.io/blog/posts/cs/ml/simons_ml_bootcamp.html</id>
    <published>2017-01-23T00:00:00Z</published>
    <updated>2017-01-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Foundations of Machine Learning Boot Camp (Simons Workshop)</h1>
    </div>
    
    <div class="info">
       <div class="subtitle"><p>Notes</p></div> 
       
        <p>Posted: 2017-01-23 
          , Modified: 2017-01-23 
	</p>
      
       <p>Tags: <a href="/tags/machine%20learning.html">machine learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>I’m currently attending the <a href="https://simons.berkeley.edu/programs/machinelearning2017">Foundations of Machine Learning</a> program at the Simons Institute.</p>
<p>Here are my <a href="https://www.dropbox.com/s/cbwmt7i2o9p0ki0/simons_ml.pdf?dl=0">notes</a> for the first workshop (Foundation of Machine Learning Boot Camp). Source is available on <a href="https://github.com/holdenlee/simons-ml">github</a>.</p>
<p>I’m also taking notes on seminars and reading groups at the Simons Program in Machine Learning. All notes are available on the <a href="http://simons.squidhive.net/Notes">Simons Program wiki</a>.</p>
<p>I will update this page as the workshop progresses.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
